# 5 自动求导

[TOC]

> [!IMPORTANT]
>
> **B站视频链接 [5 自动求导](https://www.bilibili.com/video/BV1KA411N7Px?spm_id_from=333.788.recommend_more_video.0&vd_source=31a1c76ddc1eaa699828c211cc19a5cc)**



---

## 0 引言

> 说个题外话，这个章节好难懂，看了好久也不是特别的理解，究竟在说些什么玩意儿，只能硬着头皮听下去了。

由于在进行深度学习神经网络的计算过程中对求导数这个运算较为重要因此，这里就需要进行实现自动求导这个功能，来帮助进行运算，这个玩意儿应该算是一个基础运算，在深度学习之中。

在使用前我们需要重新复习一下之前求导数的方法，也就是极为知名的**链式求导法则**。

我们文中提到的全部向量在第一次定义的时候均是列向量，也就是站着的那种向量类型



---

## 1 向量的链式法则

对于标量而言他的链式求导法则是比较简单明了的，就用下面这个例子表示一下。

![image-20250821152507841](https://raw.githubusercontent.com/sxq12320/typora_book/main/img/20250821152507895.png)

现在我们将标量的链式求导法则扩展到向量进行求解。

![image-20250821153342526](https://raw.githubusercontent.com/sxq12320/typora_book/main/img/20250821153342606.png)

可以看到引入向量之后完全不一样了。

### 1.1 情况1

![image-20250821155902212](https://i0.hdslb.com/bfs/openplatform/0d5f6c8766c0ec8f83deb97cf62d0235a0b643e3.png)

### 1.2 情况2

![image-20250821160646111](https://i0.hdslb.com/bfs/openplatform/1298c17819e613b7f6024db9145c830a141bfe36.png)

### 1.3 情况3

![image-20250821160718227](https://i0.hdslb.com/bfs/openplatform/de4d994b1a5f9898a9b1bfd75294244d5a596832.png)

上面三种情况从简单到难依次罗列了向量求导过程中的形状和大小，现在对向量的链式求导法则应该是了熟于胸了，那么现在就开始试着采用这个求导法则来真的试试求导的几个例子吧



---

## 2 链式求导实例

### 2.1 例1

**题目：**$\text{令}\vec{x}和\vec{w}均为n行的列向量,即\vec{x},\vec{w} \in R^n。y是一个标量，即y \in R 。令z = (<\vec{x} , \vec{w}> - y)^2 ，求\frac{\partial z}{\partial \vec{w}}。$

首先让我们分析一下这个题目，他让我们求导数，因此考虑采用链式求导法则进行求解。注意尖括号里面这两个向量做的是点乘，也就是内积哈。

首先先定义一些中间变量：
$$
令： \left\{ \begin{array}{l}
	a\ =\ <\vec{x},\vec{w}>\\
	b\ =\ a-y\\
	z\ =\ b^2\\
\end{array} \right.
\tag{2-1}
\\
$$
则可以将进行链式求导法则得到
$$
\begin{align}
\frac{\partial z}{\partial \vec{w}} &= \frac{\partial z}{\partial b}\times \frac{\partial b}{\partial a} \times \frac{\partial a}{\partial \vec{x}} \\
&=\frac{\partial b^2}{\partial b}\times \frac{\partial (a-y)}{\partial a}\times \frac{\partial (<\vec{x},\vec{w}>)}{\partial \vec{w}} \\ 
& = 2b \times 1 \times \vec{x}^T \\
& = 2(a-y)\vec{x}^T \\
&=2(<\vec{x},\vec{w}>-y)\vec{x}^T
\end{align}
\tag{2-2}
$$

### 2.2 例2

**题目：**$令X \in R^{m \times n} , \vec{w} \in R^n , \vec{y} \in R^m , z =||X\vec{w}-y||^2,求\frac{\partial z}{\partial \vec{w}}。$

首先和前面一样的方法先搞几个中间变量来，将z尽可能的规划的简单一些。
$$
令：\left \{\begin{array}{l}
a&=X\vec{w} \\
b&=a-\vec{y} \\
z&=||b||^2
\end{array}
 \right.
 \tag{2-3}
$$
现在就可以采用**伟大的**链式法则来求解一下这个题目了
$$
\begin{align}
\frac{\partial z}{\partial \vec{w}} &= \frac{\partial z}{\partial b}\times \frac{\partial b}{\partial a} \times \frac{\partial a}{\partial \vec{x}} \\
&=\frac{\partial ||b||^2}{\partial b}\times \frac{\partial (a-\vec{y})}{\partial a}\times \frac{\partial(X\vec{w})}{\partial \vec{w}} \\  
&=2b^T\times 1 \times X \\
&=2(a-\vec{y})^T X \\
& = 2(X\vec{w}-\vec{y})^TX
\end{align}
\tag{2-4}
$$
到这里，这两个举例应该让链式法则求导数这件事情变得比较的清晰了一些，但是我们总不能一直用手来算吧，一直用手算还叫什么自动求导呀！

现在就开始进行计算机的求导方法。

## 3 自动求导的代码实现

### 3.1 计算图原理

在计算机里面进行求导，其原理和我们手动求导的原理是一样的，也是利用中间变量配合链式法则进行求导，他的求导过程只需要下面两个东西。

1. 计算的代码块也就是操作子。
2. 计算无环图片。

那么我们就拿着**例1**的题目进行举例说明。

以这个题目为例进行求解的过程主要分成了三个步骤

1. $\vec{w}和\vec{x}产生中间变量a$ 。
2. $a和y产生中间变量b$ 。
3. $b产生应变量z$

那么我们可以画出他的计算图（无环的）

![image-20250821165939488](https://i0.hdslb.com/bfs/openplatform/2cb7b08045b0154596ded678c1e980423c004573.png)

在计算机里面便是如此
